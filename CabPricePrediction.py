# -*- coding: utf-8 -*-
"""Arpan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gfeNnXNIPCRI-Z3acflnc3SiEGuZhh0F
"""

pip install scikit-optimize

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from datetime import datetime
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor, RandomForestRegressor
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_regression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.ensemble import AdaBoostRegressor
import lightgbm as lgb
from skopt import BayesSearchCV
from lightgbm import LGBMRegressor
from sklearn.model_selection import train_test_split



df=pd.read_csv('/content/cab_rides.csv')
weather=pd.read_csv('/content/weather.csv')

df.head()
weather.head()

df.info()
weather.info()

weather['time_stamp']=[datetime.fromtimestamp(i) for i in weather['time_stamp']]
weather['time_stamp']=weather['time_stamp'].values.astype('datetime64[h]')
df['time_stamp']=[datetime.fromtimestamp(i/1000.0) for i in df['time_stamp']]
df['time_stamp']=df['time_stamp'].values.astype('datetime64[h]')
df.rename(columns={'source':'location'},inplace=True)
weather.drop_duplicates(['time_stamp','location'],inplace=True)
new_df=pd.merge(df,weather,on=['location','time_stamp'])
new_df['hour']=new_df['time_stamp'].dt.hour
new_df['day']=new_df['time_stamp'].dt.day_name()

# Perform EDA
# numeric_vars = ["distance", "temp", "clouds", "pressure", "rain", "humidity", "wind", "hour"]
# sns.pairplot(new_df, vars=numeric_vars, hue="cab_type")
# plt.show()
# new_df.isnull().sum()

new_df.drop(labels='id',axis=1,inplace=True)
new_df.dropna(subset=['price'], inplace=True)
print(new_df.info())
new_df.fillna(method='ffill', inplace=True)
new_df.dropna(axis=0,inplace=True)
print(new_df.info())

new_df.info()

# Convert categorical variables to numerical
categorical_columns = ['cab_type', 'destination', 'location', 'name', 'day']
categorical_columns_to_encode = [col for col in categorical_columns if col in new_df.columns]
new_df = pd.get_dummies(new_df, columns=categorical_columns_to_encode)
print(new_df)

# Split the data into train and test sets

X = new_df.drop(columns=["price", "time_stamp", "product_id"])
y = new_df["price"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.impute import SimpleImputer
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.metrics import mean_squared_error
# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_train_imputed = imputer.fit_transform(X_train)
X_test_imputed = imputer.transform(X_test)

param_space = {
    'learning_rate': [0.05, 0.1, 0.2],       # Discrete values for learning rate
    'n_estimators': [50, 100, 200],          # Discrete values for number of estimators
    'num_leaves': [10, 25, 40],              # Discrete values for number of leaves
    'reg_lambda': [0.01, 0.1, 1, 10],        # Discrete values for regularization lambda
    'boosting_type': ['dart', 'rf']          #
}

lgbm_model = lgb.LGBMRegressor(random_state=42, bagging_freq=5, bagging_fraction=0.8, feature_fraction=0.8, feature_name=X_train.columns.tolist()) # learning_rate=0.05, n_estimators=2000, max_leaves=40, reg_lambda=0.1, boosting_type='dart')

# Perform Bayesian optimization
opt = BayesSearchCV(
    lgbm_model,
    param_space,
    n_iter=25,               # Number of parameter settings that are sampled
    cv=3,                    # Number of cross-validation folds
    random_state=42          # Random state for reproducibility
)

# Fit the optimizer to the data
opt.fit(X_train_imputed, y_train)

best_model = opt.best_params_
best_score = opt.best_score_

print("Best Parameters:", opt.best_params_)

# Evaluate the model using cross-validation
cv_scores = cross_val_score(opt.best_estimator_, X_train, y_train, cv=5)

print("Best Parameters:", best_model)
print("Best Score:", best_score)
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Score:", cv_scores.mean())



# Evaluate the model using cross-validation
cv_scores = cross_val_score(opt.best_estimator_, X_train, y_train, cv=5)

# Make predictions
y_pred_train = opt.best_estimator_.predict(X_train_imputed)
y_pred_test = opt.best_estimator_.predict(X_test_imputed)

# Calculate error
train_error = mean_squared_error(y_train, y_pred_train)
test_error = mean_squared_error(y_test, y_pred_test)

print("Train MSE:", train_error)
print("Test MSE:", test_error)

# Calculate R-squared for training and test predictions
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print("Train R-squared:", train_r2)
print("Test R-squared:", test_r2)
print("Cross-Validation Scores:", cv_scores)
print("Mean Cross-Validation Score:", cv_scores.mean())

for i, column in enumerate(X.columns):
    print(f"{i}: {column}")

# Get the best estimator from the optimization results
best_estimator = opt.best_estimator_

# Train the best estimator on the entire dataset
best_estimator.fit(X_train_imputed, y_train)

# Plot feature importance
lgb.plot_importance(best_estimator, max_num_features=10, figsize=(10, 6))
plt.show()

plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred_test, color='blue', alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--', lw=2)
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Scatter Plot of Predicted vs. Actual Prices for LGBM')
plt.grid(True)
plt.show()

# plt.figure(figsize=(8, 6))
# plt.scatter(y_test, y_pred_test, alpha=0.5)
# plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red')
# plt.xlabel('Actual Prices')
# plt.ylabel('Predicted Prices')
# plt.title('Residual Plot')
# plt.show()

residuals = y_test - y_pred_test

# Plot residual plot
sns.residplot(x=y_pred, y=residuals, lowess=True, color="g")
plt.xlabel("Predicted values")
plt.ylabel("Residuals")
plt.title("Residual Plot")
plt.show()

# Distribution Comparison
plt.figure(figsize=(10, 6))
sns.kdeplot(y_test, label='Actual Prices', color='blue', shade=True)
sns.kdeplot(y_pred_test, label='Predicted Prices', color='red', shade=True)
plt.xlabel('Price')
plt.ylabel('Density')
plt.title('Distribution Comparison of Predicted vs. Actual Prices')
plt.legend()
plt.show()

from sklearn.metrics import mean_absolute_error, mean_squared_error
mae = mean_absolute_error(y_test, y_pred_test)

# MSE
mse = mean_squared_error(y_test, y_pred_test)

# RMSE
rmse = np.sqrt(mse)

# Explained Variance Score
# explained_variance = explained_variance_score(y_test, y_pred_test)

# Print the metrics
print("Mean Absolute Error (MAE):", mae)
# print("Mean Squared Error (MSE):", mse)
print("Root Mean Squared Error (RMSE):", rmse)
print("Explained Variance Score:", explained_variance)

from sklearn.metrics import r2_score

# Calculate R-squared for training and test predictions
train_r2 = r2_score(y_train, y_pred_train)
test_r2 = r2_score(y_test, y_pred_test)

print("Train R-squared:", train_r2)
print("Test R-squared:", test_r2)
r2_score(y_test,gb_pred)

plt.scatter(y_test, y_pred_test, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)

plt.xlabel('Actual Values (y)')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values in HistGradientBoostingRegressor')
plt.show()

from sklearn.ensemble import BaggingRegressor
new_df.dropna(inplace=True)


# Bagging (BaggingRegressor)
bagging_model = BaggingRegressor()
bagging_model.fit(X_train, y_train)
bagging_pred = bagging_model.predict(X_test)
bagging_rmse = np.sqrt(mean_squared_error(y_test, bagging_pred))
print("Bagging RMSE:", bagging_rmse)

r2_score(y_test,bagging_pred)

plt.scatter(y_test, bagging_pred, color='blue')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle='--', color='red', linewidth=2)

plt.xlabel('Actual Values (y)')
plt.ylabel('Predicted Values')
plt.title('Actual vs Predicted Values in HistGradientBoostingRegressor')
plt.show()